---
layout: post
title: '[구글 머신러닝 부트캠프 2024] 참여 후기'
category: ml
tags: [ml]
date: 2024-10-03
image: "/assets/img/ml/mlb/mlb_banner.webp"
message: ".."
excerpt: ".."
---

- table of contents
{:toc}

- 7월 1일부터 시작하여 10월 4일로 마무리된 **구글 머신러닝 부트캠프 2024**의 후기를 작성해볼까 한다.

## 지원과 합격

- 사실 구글 머신러닝 부트캠프(이하 MLB)에 지원하게 된 것이 이번 해가 처음은 아니었다. 작년의 **MLB 2023**에도 지원서를 제출했으나, 지원 단계에서 **불합격 통보**를 받은 바 있다. 그때는 아무래도 42서울의 교육과정을 진행하고 있던 도중인데다가 자기소개서의 상태도 그렇게 좋지 못했기 때문에 합격하지 못한 것이 당연했다고 생각하고, 아마 합격했어도 원래 하던 것들 때문에 제대로 된 성과를 내지 못했을 것이다. 차라리 올해 **MLB 2024**에 참여할 수 있어서 다행이었던 것 같다.

- 사실 인공지능에 아주 큰 뜻이 있어서 지원한 것은 아니었다. **무엇보다도 태생 문과에, 독어독문학과를 졸업해서 다른 부트캠프에서 겨우 C, C++이나 좀 만져본 내가 인공지능을 공부해서 그쪽으로 진로를 잡을 수나 있을까?** 하는 마음이 컸다. 그래서 자기소개서를 작성하면서도 내심 떨어지겠거니하고 별 기대를 걸지 않았는데, 합격 통보를 받고 나서는 어떻게 붙은걸까 한동안 어리둥절했었다.

<center><img src="/assets/img/ml/mlb/gamsa.webp" width="60%" height="60%"></center>

<span style="color:#808080">(구글 코리아에... 감사하지 않으면 안되겠지...?)</span>

- 지원서의 경우, 자기소개와 더불어 **기본적인 Python 프로그래밍 문제**를 풀어 제출했어야 했다. 나는 MLB를 지원하기 전까지 제대로 Python을 다뤄본 경험이 없었기 때문에, 자기소개서를 쓰면서도 애를 좀 먹었었다. 지원자가 많은 만큼 문제를 거의 다 맞추듯 해야 합격권이라는 소문이 있었는데, 합격하고 나서 확인하기로는 평균이 아주 높지는 않았다고 한다. 풀면서도 아리까리하고 쉽지 않다고 생각했는데 역시 나에게 어려우면 남에게도 어렵다고, 그런게 아니었을까 싶다.

<center><img src="/assets/img/ml/mlb/invi.webp" width="100%" height="100%"></center><br>

- 합격하고 난 이후에 온 초대장에서는 어떤 과정이 진행되는지에 대한 정보와 테크톡, 멘토링 세션의 진행에 대한 알림이 들어있었다. 내가 저런 일정들을 소화할 수 있을까 두려움이 있었지만, 스터디원들과 함께 공부하면서 시간을 쏟다보니 어떻게 모든 과정을 마무리할 수 있었다. 역시 해보지 않고서는 모르는 것이 아닌가 싶다.


## 프로그램

- 이번 MLB 2024는 크게 **코세라 Deep Learning Specialization 교육과정 수강, 캐글 미션 선택 달성(Playground/Competition), Gemma Sprint 완주의 세 가지 과정**으로 구성되어 있었다. 

### (1) Deep Learning Specialization 교육 과정 수강

<br><center><img src="/assets/img/ml/mlb/dls_capture.webp" width="70%" height="70%"></center><br>

<center><span style="color:#808080">어떻게든 마무리짓고 링크드인에 인증하였다...</span></center><br>

- 7월 한달과 8월 초까지 **Cousera**에서 제공되는 Andrew Ng 선생님의 **Deep Learning Specialization**을 들으며 신경망과 딥러닝에 대한 기초 지식을 습득하는 시간을 가졌다. 다섯 개의 챕터로 이루어졌고, 챕터 당 1~4주차 강의들이 존재했다. 스터디원들과 함께 공부하며 강의를 수강했고, 일주일에 챕터 하나를 완주하는 것을 목표로 했다.

- 후반부 챕터의 경우, 난이도가 꽤 있는 편이어서 진도를 빼기가 조금 어려웠다. 강의 이외로도 공부를 해가면서 진행했으면 더 좋았겠지만, 7월엔 아직 42서울 과정이 마무리되지 않아 온전히 강의에 몰입하지 못했다. 이 부분은 조금 아쉽다고 생각한다.

- 8월부터 Kaggle을 시작하고 되고, 9월엔 Gemma Sprint를 하게 되는데 이쯤 되면 강의의 내용들이 거의 기억이 나지 않는다. 그렇게 될 걸로 예상하고 정리본을 만들어두었지만 이후에는 위의 둘에 시간을 쓰느라 복기할 시간을 갖지 못했다. 나중에 복습의 시간을 가져야 할 것 같다.

- 여담으로 학습은 거의 **유튜브 영상**을 통해 이루어졌다. Cousera에서 자체적으로 자막이 제공되기는 했지만, 아무래도 번역기를 써서 번역한 모양인지 퀄리티가 아주 뛰어나지는 않았다. 유튜브의 **Deeplearning AI** 채널의 자막이 더 번역이 잘되어 있어 그쪽을 이용했다. 특히 train set가 **훈련 세트**가 아니라 **열차 세트**로 번역되어 있었던 것이 기억에 남는다. 이거는 사실 영어를 잘하면 별 문제가 없는 부분이라고 생각하기는 한다.

### (2) Kaggle Competition 참가

<br><center><img src="/assets/img/ml/mlb/rank2.webp" width="100%" height="100%"></center><br>

<center><span style="color:#808080">638등 / 1823팀으로 기준인 40%의 살짝 상위인 35%로 턱걸이 마무리</span></center><br>

- 사실상 MLB 2024의 꽃이나 다름 없는 프로그램이었다고 생각한다. 주어진 데이터셋으로 전처리를 하고, 더 나은 예측을 하는 코드를 만들어 다른 사람과 경쟁하는 데이터 사이언스 커뮤니티 사이트이다. 나는 이번에 처음으로 Kaggle에 참가하게 되었다.

- 캐글 미션을 완료하기 위해서 두 가지 옵션 중 하나를 선택할 수 있었다. 첫 번째로는 난이도가 낮은 **Playground** 종목에서 상위 20% 이내의 성적을 내거나, 두 번째로 비교적 난이도가 높은 **Competition** 종목에서 상위 40% 이내의 성적을 내는 것이다. 나는 함께 공부하던 스터디원들과 함께 야심차게 Competition의 40% 안에 드는 것을 목표로 잡았다.

- 다만, Python에 익숙하지 않다는 점이 발목을 잡았다. 다른 문제들도 많았지만 **특히 파이썬의 데이터 처리 모듈인 pandas에 대한 지식의 부재가 대회를 진행하는데 큰 애로사항이 되었다**. 결국 모델을 호출하기 위해서는 데이터의 전처리가 전제되어야 하는데, pandas를 잘 모르니 데이터를 자유자재로 가공하는 것에 어려움을 겪었다. 코드를 쳐다봐도 잘 이해가 되지 않으니 마음이 급해져서 기본기를 익히기 보다는 그때그때 마구잡이로 메서드를 쑤셔넣고, 또 지나면 까먹기를 반복하며 시행착오를 겪었다. 

- 특히 고통스러웠던 점은, 파이썬 기본기나 다른 모듈들에 대해 공부하려고 하면 그 자체가 대회에 시간을 투자하는 것이니 진도를 빼는게 아니라 혹여나 스터디원들에게 민폐가 되지는 않나 불안하고 그렇다고 다른 사람들이 작성한 Code를 살펴보자니 기본기가 부족해서 잘 알아들을 수 없으니 하염없이 코드만 바라보면서 진행이 잘 되지 않는 것이었다. 지금 와서 되돌아보면 너무너무 아쉽지만, 8월엔 이것 때문에 꽤 골이 아팠었다. 

- 결국 이도저도 못한 채, 스터디원들이 작성한 코드의 매개 변수를 튜닝하면서 캐글을 마무리했다. 이번 부트캠프의 가장 중요한 부분을 이렇게 어물쩡 넘어가게 되어 너무 아쉬웠다. 차후에 혼자 Playground라도 진행하면서 반드시 기초를 다지고 넘어가야 겠다고 다짐했다.

<br><center><img src="/assets/img/ml/mlb/guide.webp" width="60%" height="60%"></center><br>

- 이후에 알게 된 사실인데, Kaggle에서도 자체적으로 가이드를 제시해준다. 이런 것부터 천천히 읽으면서 시작해 보았다면 무언가 달랐을까? 아무튼 너무 마음을 급하게 먹고 허둥지둥 한 달을 보냈었던 것 같아 아쉽다. ~~시즌 54321호 아쉽다ㅠㅠ~~

### (3) Gemma Sprint 참가

<center><img src="/assets/img/ml/mlb/gemma_capture.webp" width="60%" height="60%"></center><br>

<center><span style="color:#808080">모델을 파인 튜닝한 후 소셜 미디어로 인증!</span></center><br>

[[구글 머신러닝 부트캠프 2024] Gemma Sprint, Gemma2를 파인튜닝 해보자](https://espebaum.github.io/ml/Gemma2FineTuning)

- 9월부터 **Gemma Sprint**를 진행하게 되었다. 이번에 새로 생긴 프로그램인데, 구글의 Gemma2를 파인튜닝해서 배포하고 App이나 Code를 작성하거나, 블로그나 유튜브로 Tutorial을 만들어보는 프로젝트이다. **자세한 내용과 프로젝트의 진행 경과는 링크에 적혀있으니 그 쪽을 참고**하면 좋을 것 같다.

- 나는 Gemma Sprint에 많은 시간을 투자했는데, Kaggle에서 좋은 성과를 내지 못했기 때문에 Gemma Sprint에서나마 성과를 내보고자 하는 생각이 강했기 때문이다. 구글에 Gemma 모델을 파인 튜닝하는 방법을 알려주는 잘 정리된 채널이 있었고, 따라가기가 쉬웠기 때문에 더 매달려서 진행했었다. 결과적으로 아주 못봐줄 정도는 아닌 모델을 하나 튜닝할 수 있었고, 당시에 아주 바닥을 치던 자존감을 조금이나마 높일 수 있었다. 

- 가진 GPU가 없기도 하고, **Colab이나 Kaggle**에서 Gemma 모델을 아주 쉽게 호출할 수 있도록 여러 메서드를 지원해주다 보니 자연스럽게 Colab으로 Gemma2를 파인 튜닝하게 되었다. 그런데 아무래도 LLM을 만지는 것이 처음이다 보니 생각보다 많은 돈을 지출하게 되어, 확실히 LLM이 개인이 접근하기가 그렇게 쉽지만은 않은 영역이라는 것을 실감했다. 나중에 머신러닝이나 딥러닝을 제대로 하게 된다면 좋은 GPU를 가진 데스크탑을 하나 마련해야 하지 않을까 싶다. **좋은 GPU가 곧 국력**이라는 것을 깨닫는 시간이었다.

## 마치며...

- 기존에 진행하던 42서울 교육과정을 끝마친 이후에도 진로를 확실하게 정하지 못하고 어물쩡거리고 있었는데, 마침 구글에서 이런 기회를 제공해줘서 귀중한 경험을 할 수 있었다. 함께 공부했었던 스터디원들도 모두 의욕이 넘치고, 실력이 좋아서 수월하게 부트캠프를 진행할 수 있었다(~~승객으로~~). 헤매는 동안에는 많이 힘들었지만, 갈무리하면서 되돌아보니 결과적으로는 행복한 시간이었다고 생각한다. 언제까지고 아쉬워할수만은 없으니, 나중을 위한 좋은 경험을 했다고 생각해야할 것 같다. 

- 후속 과정으로 **NIPA-Google ML 부트캠프 협력 실무 프로젝트(4기)**가 있는 모양이던데, 가능하다면 그것도 한번 해보고 싶다. 그리고 이쪽으로 가능성이 보인다면, 대학원 진학도 고려해보고 싶다. 부트캠프를 진행하며 부족했던 부분들을 채워가며 미래에 대해 진지하게 생각해보는 시간을 가져야겠다... (NIPA 실무 프로젝트 참여 후기는 [이쪽](https://espebaum.github.io/ml/GoogleNipa))

- 10월 18일에 졸업식이 예정되어 있는데, 거기서 사진도 찍고 하면서 글을 좀 풍성하게 꾸미고 마무리할 듯 싶다.